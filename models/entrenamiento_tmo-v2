# =================================================================================
# TRAIN TMO (KAGGLE) v8-DIRECTO-AFINADO (¡LÓGICA FINAL!)
# - Estrategia: Directa (igual que el Planner de Llamadas)
# - Objetivo: "tmo_s" (Valor Total)
# - NO USA BASELINE, NO USA RESIDUAL.
# - Loss Function: MSE (para "perseguir" picos y valles).
# - Features: Tiempo + Lags/MAs/STD(TMO Total) + Lags/MAs/STD(Volumen) + Contexto
# --- MEJORAS (AFINADO FINAL) ---
# 1. (NUEVO) Agrega "sensores de volatilidad" (STD) a TMO Total y Volumen.
# 2. (NUEVO) "Suelta los frenos": reduce Dropout y L2 para predicciones más agresivas.
# =================================================================================

import os, re, json, warnings, unicodedata
from pathlib import Path

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

# ---------------- Config Kaggle ----------------
KAGGLE_INPUT_DIR = "/kaggle/input/data-ia/"
HOSTING_FILE = os.path.join(KAGGLE_INPUT_DIR, "Hosting ia.xlsx")
OUTPUT_DIR = "/kaggle/working/models/"

EPOCHS = 120
BATCH_SIZE = 256
SEED = 42
FERIADOS_COL = "feriados"

# nombres aceptados para la columna objetivo
TARGET_TMO_COL_RE = r"^tmo\s*\((s|segundos)\)$"

# Mantenemos 'True' (decisión del paso anterior)
USE_SAMPLE_WEIGHT = True 
CONTESTADAS_CANDIDATES = ["contestadas", "q_contestadas", "llamadas_contestadas"]

# Columnas de contexto (proporciones) que podrían estar en el Excel
CONTEXT_COL_CANDIDATES = [
    "proporcion_comercial", "proporcion_tecnica",
    "tmo_comercial", "tmo_tecnico"
]


np.random.seed(SEED)
tf.random.set_seed(SEED)
warnings.filterwarnings("once", category=UserWarning)

# ---------------- Utils (Sin cambios) ----------------
def read_any(path: str, sheet: int | str | None = None) -> pd.DataFrame:
    p = str(path).lower()
    if p.endswith(".csv"):
        try:
            df = pd.read_csv(path, low_memory=False)
            if df.shape[1] == 1 and ';' in str(df.iloc[0,0]):
                df = pd.read_csv(path, sep=';', low_memory=False)
            return df
        except Exception:
            return pd.read_csv(path, sep=';', low_memory=False)
    if p.endswith(".xlsx") or p.endswith(".xls"):
        return pd.read_excel(path, sheet_name=sheet if sheet is not None else 0)
    raise ValueError(f"Formato no soportado: {path}")

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def ensure_ts_from_hosting(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d.columns = [c.strip().lower().replace(" ", "_") for c in d.columns]
    fecha_col = next((c for c in d.columns if "fecha" in c), None)
    hora_col  = next((c for c in d.columns if "hora"  in c), None)
    if not fecha_col or not hora_col:
        raise ValueError("Se requieren columnas con 'fecha' y 'hora' para construir 'ts'.")
    d["ts"] = pd.to_datetime(d[fecha_col].astype(str) + " " + d[hora_col].astype(str),
                             errors="coerce", dayfirst=True)
    d = d.dropna(subset=["ts"]).sort_values("ts").reset_index(drop=True)
    return d

def find_tmo_col(df: pd.DataFrame) -> str:
    norm_map = {_norm(c): c for c in df.columns}
    for n, orig in norm_map.items():
        if re.match(r"^tmo\s*\(\s*(s|segundos)\s*\)$", n):
            return orig
    for n, orig in norm_map.items():
        if "tmo" in n and ("(s)" in n or "segundo" in n):
            return orig
    raise ValueError("No se encontró columna objetivo: usa 'TMO (s)' o 'TMO (segundos)'.")

def add_time_parts(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d["dow"]   = d["ts"].dt.dayofweek
    d["month"] = d["ts"].dt.month
    d["hour"]  = d["ts"].dt.hour
    d["day"]   = d["ts"].dt.day
    d["sin_hour"] = np.sin(2*np.pi*d["hour"]/24.0)
    d["cos_hour"] = np.cos(2*np.pi*d["hour"]/24.0)
    d["sin_dow"]  = np.sin(2*np.pi*d["dow"]/7.0)
    d["cos_dow"]  = np.cos(2*np.pi*d["dow"]/7.0)
    d["es_dia_de_pago"] = d["day"].isin([1,2,15,16,29,30,31]).astype(int)
    return d

def find_col_by_candidates(df: pd.DataFrame, candidates: list) -> str | None:
    cols_norm = {_norm(c): c for c in df.columns}
    for cand in candidates:
        n = _norm(cand)
        for k, orig in cols_norm.items():
            if k == n or k.replace("_", " ") == n:
                return orig
    # Heurística
    if "contestadas" in candidates:
         for k, orig in cols_norm.items():
            if "contest" in k or ("recibidos" in k and "nacional" in k):
                return orig
    return None

# ---------------- Modelo ----------------
def create_model(n_features: int) -> tf.keras.Model:
    
    # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 1 - "Soltar Frenos") ---
    l2_reg = 1e-6 # (Era 1e-5)
    
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(n_features,)),
        tf.keras.layers.Dense(256, activation='relu',
                               kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(0.20), # (Era 0.25)
        tf.keras.layers.Dense(128, activation='relu',
                               kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(0.15), # (Era 0.20)
        tf.keras.layers.Dense(64, activation='relu',
                               kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dense(1, activation='linear'),
    ])
    # --- FIN MODIFICACIÓN ---
    
    lr = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9
    )
    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)

    # (Mantenemos MSE, que es la estrategia correcta para "Directo")
    loss_function = 'mean_squared_error' 
    print(f"INFO: Usando función de pérdida: {loss_function}")
    model.compile(optimizer=opt, loss=loss_function, metrics=['mae'])
    
    return model

# ---------------- Entrenamiento ----------------
def train_tmo_from_hosting_kaggle_direct():
    print("\n" + "="*70)
    print("ENTRENAMIENTO TMO v8-DIRECTO-AFINADO | objetivo = 'TMO (s)'")
    print("="*70)

    # 1) Cargar Hosting
    if not os.path.exists(HOSTING_FILE):
        raise FileNotFoundError(f"No se encuentra el archivo: {HOSTING_FILE}")
    raw = read_any(HOSTING_FILE, sheet=0)

    # 2) ts + columnas clave
    d = ensure_ts_from_hosting(raw)

    # feriados
    if FERIADOS_COL not in d.columns:
        cand = [c for c in raw.columns if _norm(c).replace("_"," ") == FERIADOS_COL]
        if cand:
            d[FERIADOS_COL] = pd.to_numeric(raw[cand[0]], errors='coerce').fillna(0).astype(int)
        else:
            raise ValueError(f"No se encontró columna '{FERIADOS_COL}'.")
    else:
        d[FERIADOS_COL] = pd.to_numeric(d[FERIADOS_COL], errors='coerce').fillna(0).astype(int)

    # TMO objetivo
    tmo_col = find_tmo_col(raw)
    d = d.join(pd.to_numeric(raw[tmo_col], errors="coerce").rename("tmo_s"))
    d["tmo_s"] = pd.to_numeric(d["tmo_s"], errors="coerce")

    # (opcional) columna contestadas (para features y/o sample_weight)
    contest_col = find_col_by_candidates(raw, CONTESTADAS_CANDIDATES)
    if contest_col is not None:
        print(f"INFO: Usando '{contest_col}' como columna de volumen (contestadas).")
        d = d.join(pd.to_numeric(raw[contest_col], errors="coerce").rename("contestadas"))
    else:
        print("WARN: No se encontró columna de 'contestadas'. Los features de volumen serán 0.")
        d["contestadas"] = 0

    # Cargar Contexto
    print("INFO: Buscando columnas de contexto (proporciones, tmo_segmento)...")
    loaded_context_cols = []
    raw_cols_norm = {_norm(c): c for c in raw.columns}
    for cand_norm in CONTEXT_COL_CANDIDATES:
        if cand_norm in raw_cols_norm:
            orig_col = raw_cols_norm[cand_norm]
            print(f"INFO: Cargando columna de contexto: '{orig_col}'")
            d[cand_norm] = pd.to_numeric(raw[orig_col], errors='coerce')
            d[cand_norm] = d[cand_norm].ffill().bfill().fillna(d[cand_norm].mean()) # Relleno robusto
            loaded_context_cols.append(cand_norm)

    # 3) partes de tiempo
    d = add_time_parts(d)
    
    # (Lógica Residual eliminada)

    # 4) autorregresivas (Bloque de 2 tipos de features)
    
    # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 2 - "Sensor de Volatilidad") ---
    # 4a) PISTA 1: Lags/MAs/STD del TMO TOTAL
    print("INFO: Creando features Pista 1 (TMO Total) con lags RÁPIDOS y STD...")
    s_tmo_total = d["tmo_s"]
    for lag in [1, 2, 3, 6, 12, 24, 168]:
        d[f"lag_tmo_total_{lag}"] = s_tmo_total.shift(lag)
    s_tmo_total_s1 = s_tmo_total.shift(1)
    for w in [6, 12, 24, 72]:
        d[f"ma_tmo_total_{w}"] = s_tmo_total_s1.rolling(w, min_periods=1).mean()
        d[f"std_tmo_total_{w}"] = s_tmo_total_s1.rolling(w, min_periods=2).std() # <-- SENSOR AÑADIDO

    # 4b) PISTA 2: Lags/MAs/STD de VOLUMEN
    print("INFO: Creando features Pista 2 (Volumen/Contestadas) con STD...")
    s_contest = pd.to_numeric(d["contestadas"], errors="coerce").fillna(0)
    for lag in [1, 24, 48, 168]:
         d[f"lag_contest_{lag}"] = s_contest.shift(lag)
    s_contest_s1 = s_contest.shift(1)
    for w in [6, 24, 72]:
        d[f"ma_contest_{w}"] = s_contest_s1.rolling(w, min_periods=1).mean()
        d[f"std_contest_{w}"] = s_contest_s1.rolling(w, min_periods=2).std() # <-- SENSOR AÑADIDO
    # --- FIN MODIFICACIÓN ---


    # 5) features (Lista de features combinada)
    cyc_feats   = ["sin_hour","cos_hour","sin_dow","cos_dow", FERIADOS_COL, "es_dia_de_pago", "dow","month","hour"]
    
    # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 2 - Agregar STD a la lista) ---
    tmo_total_feats = [c for c in d.columns if c.startswith(("lag_tmo_total_", "ma_tmo_total_", "std_tmo_total_"))]
    contest_feats   = [c for c in d.columns if c.startswith(("lag_contest_", "ma_contest_", "std_contest_"))]
    # --- FIN MODIFICACIÓN ---
    context_feats   = loaded_context_cols 

    features = (
        cyc_feats + tmo_total_feats + contest_feats + context_feats
    )
    
    print(f"\nINFO: Total features a usar: {len(features)}")

    # 6) dataset modelable (Target es 'tmo_s')
    dm = d.dropna(subset=features + ["tmo_s"]).copy()
    X = pd.get_dummies(dm[features], columns=["dow","month","hour"], drop_first=False)
    y = dm["tmo_s"].loc[X.index].astype(float)

    # sample_weight (opcional, no es feature)
    if USE_SAMPLE_WEIGHT and "contestadas" in dm.columns:
        print("INFO: Usando SAMPLE WEIGHT (ponderado por 'contestadas')")
        sw = dm["contestadas"].loc[X.index].fillna(0).astype(float)
        sw = np.sqrt(np.clip(sw, 0, np.percentile(sw[sw>0], 99))) + 1.0  # suavizado + cap p99
    else:
        print("INFO: NO se usará sample weight (entrenamiento uniforme).")
        sw = None

    # saneo
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    if X.isna().any().any():
        print("INFO: Rellenando NaNs en features (ffill)...")
        X = X.ffill().dropna()
    y = y.loc[X.index]
    if sw is not None:
        sw = sw.loc[X.index]
    if X.empty:
        raise ValueError("No quedaron datos para entrenar TMO tras el preprocesamiento.")

    # 7) split temporal + escalado
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, shuffle=False)
    scaler = StandardScaler()
    X_tr_s = scaler.fit_transform(X_tr)
    X_te_s = scaler.transform(X_te)
    sw_tr = sw.loc[X_tr.index].values if sw is not None else None
    sw_te = sw.loc[X_te.index].values if sw is not None else None

    # 8) modelo
    model = create_model(X_tr_s.shape[1])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, mode='min')
    model.fit(
        X_tr_s, y_tr,
        validation_data=(X_te_s, y_te, sw_te) if sw_te is not None else (X_te_s, y_te),
        sample_weight=sw_tr,
        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[es]
    )

    # 9) métricas (directas)
    yhat_sec = model.predict(X_te_s, verbose=0).flatten()
    y_true_sec = y_te.values

    mae = mean_absolute_error(y_true_sec, yhat_sec)
    r2  = r2_score(y_true_sec, yhat_sec)
    print(f"\nResultado TMO v8-DIRECTO-AFINADO: MAE={mae:.2f} | R2={r2:.3f}")
    
    print(f"Distrib y_te : med={np.median(y_true_sec):.1f} | p90={np.percentile(y_true_sec,90):.1f} | p99={np.percentile(y_true_sec,99):.1f}")
    print(f"Distrib pred: med={np.median(yhat_sec):.1f} | p90={np.percentile(yhat_sec,90):.1f} | p99={np.percentile(yhat_sec,99):.1f}")

    # 10) artefactos
    out = Path(OUTPUT_DIR); out.mkdir(parents=True, exist_ok=True)
    # columnas + scaler de features
    (out / "training_columns_tmo.json").write_text(json.dumps(list(X.columns), ensure_ascii=False, indent=2), encoding="utf-8")
    joblib.dump(scaler, out / "scaler_tmo.pkl")
    
    # (No hay baseline.csv, No hay meta.json)
    
    # modelo
    model.save(str(out / "modelo_tmo.keras"))

    print("\nArtefactos guardados en:", str(out.resolve()))
    print(" - modelo_tmo.keras")
    print(" - scaler_tmo.pkl")
    print(" - training_columns_tmo.json")
    print("\n¡Listo! Descárgalos desde la pestaña 'Output'.")

# ---------------- Run ----------------
if __name__ == "__main__":
    train_tmo_from_hosting_kaggle_direct()
