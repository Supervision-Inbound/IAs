# =================================================================================
# TRAIN TMO (KAGGLE) v7-residual-AFINADO
# - Objetivo: "tmo (segundos)" o "tmo (s)" desde Hosting ia.xlsx
# - Aprende el RESIDUO contra baseline por (dow, hour) y lo estandariza (z-score)
# --- MEJORAS (AFINADO) ---
# 1. Agrega Lags/MAs de 'contestadas' (Volumen) como feature.
# 2. Agrega Lags/MAs de 'tmo_s' (TMO Total) como feature.
# 3. MANTIENE sample_weight (USE_SAMPLE_WEIGHT = True).
# 4. (NUEVO) Usa "lags rápidos" (1h, 2h, 3h) para TMO Total.
# 5. (NUEVO) Usa loss='mean_squared_error' para "perseguir" picos/valles.
# =================================================================================

import os, re, json, warnings, unicodedata
from pathlib import Path

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
import joblib

# ---------------- Config Kaggle ----------------
KAGGLE_INPUT_DIR = "/kaggle/input/data-ia/"
HOSTING_FILE = os.path.join(KAGGLE_INPUT_DIR, "Hosting ia.xlsx")
OUTPUT_DIR = "/kaggle/working/models/"

EPOCHS = 120
BATCH_SIZE = 256
SEED = 42
FERIADOS_COL = "feriados"

# nombres aceptados para la columna objetivo
TARGET_TMO_COL_RE = r"^tmo\s*\((s|segundos)\)$"

# Mantenemos 'True' (decisión del paso anterior)
USE_SAMPLE_WEIGHT = True 
CONTESTADAS_CANDIDATES = ["contestadas", "q_contestadas", "llamadas_contestadas"]

# Columnas de contexto (proporciones) que podrían estar en el Excel
CONTEXT_COL_CANDIDATES = [
    "proporcion_comercial", "proporcion_tecnica",
    "tmo_comercial", "tmo_tecnico"
]


np.random.seed(SEED)
tf.random.set_seed(SEED)
warnings.filterwarnings("once", category=UserWarning)

# ---------------- Utils ----------------
def read_any(path: str, sheet: int | str | None = None) -> pd.DataFrame:
    p = str(path).lower()
    if p.endswith(".csv"):
        try:
            df = pd.read_csv(path, low_memory=False)
            if df.shape[1] == 1 and ';' in str(df.iloc[0,0]):
                df = pd.read_csv(path, sep=';', low_memory=False)
            return df
        except Exception:
            return pd.read_csv(path, sep=';', low_memory=False)
    if p.endswith(".xlsx") or p.endswith(".xls"):
        return pd.read_excel(path, sheet_name=sheet if sheet is not None else 0)
    raise ValueError(f"Formato no soportado: {path}")

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def ensure_ts_from_hosting(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d.columns = [c.strip().lower().replace(" ", "_") for c in d.columns]
    fecha_col = next((c for c in d.columns if "fecha" in c), None)
    hora_col  = next((c for c in d.columns if "hora"  in c), None)
    if not fecha_col or not hora_col:
        raise ValueError("Se requieren columnas con 'fecha' y 'hora' para construir 'ts'.")
    d["ts"] = pd.to_datetime(d[fecha_col].astype(str) + " " + d[hora_col].astype(str),
                             errors="coerce", dayfirst=True)
    d = d.dropna(subset=["ts"]).sort_values("ts").reset_index(drop=True)
    return d

def find_tmo_col(df: pd.DataFrame) -> str:
    norm_map = {_norm(c): c for c in df.columns}
    for n, orig in norm_map.items():
        if re.match(r"^tmo\s*\(\s*(s|segundos)\s*\)$", n):
            return orig
    for n, orig in norm_map.items():
        if "tmo" in n and ("(s)" in n or "segundo" in n):
            return orig
    raise ValueError("No se encontró columna objetivo: usa 'TMO (s)' o 'TMO (segundos)'.")

def add_time_parts(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d["dow"]   = d["ts"].dt.dayofweek
    d["month"] = d["ts"].dt.month
    d["hour"]  = d["ts"].dt.hour
    d["day"]   = d["ts"].dt.day
    d["sin_hour"] = np.sin(2*np.pi*d["hour"]/24.0)
    d["cos_hour"] = np.cos(2*np.pi*d["hour"]/24.0)
    d["sin_dow"]  = np.sin(2*np.pi*d["dow"]/7.0)
    d["cos_dow"]  = np.cos(2*np.pi*d["dow"]/7.0)
    d["es_dia_de_pago"] = d["day"].isin([1,2,15,16,29,30,31]).astype(int)
    return d

def make_tmo_autoreg_features(df: pd.DataFrame) -> pd.DataFrame:
    # Esta función (antigua) no se usa, el código está inline
    # para mayor claridad de las 3 pistas.
    pass

def find_col_by_candidates(df: pd.DataFrame, candidates: list) -> str | None:
    cols_norm = {_norm(c): c for c in df.columns}
    for cand in candidates:
        n = _norm(cand)
        for k, orig in cols_norm.items():
            if k == n or k.replace("_", " ") == n:
                return orig
    # Heurística
    if "contestadas" in candidates:
         for k, orig in cols_norm.items():
            if "contest" in k or ("recibidos" in k and "nacional" in k):
                return orig
    return None

# ---------------- Modelo ----------------
def create_model(n_features: int) -> tf.keras.Model:
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(n_features,)),
        tf.keras.layers.Dense(256, activation='relu',
                               kernel_regularizer=regularizers.l2(1e-5)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Dense(128, activation='relu',
                               kernel_regularizer=regularizers.l2(1e-5)),
        tf.keras.layers.Dropout(0.20),
        tf.keras.layers.Dense(64, activation='relu',
                               kernel_regularizer=regularizers.l2(1e-5)),
        tf.keras.layers.Dense(1, activation='linear'),
    ])
    lr = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9
    )
    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)

    # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 2) ---
    # Cambiamos de Huber a MSE para forzar al modelo a "perseguir"
    # los picos y valles (errores grandes).
    loss_function = 'mean_squared_error' 
    print(f"INFO: Usando función de pérdida: {loss_function}")
    model.compile(optimizer=opt, loss=loss_function, metrics=['mae'])
    # --- FIN MODIFICACIÓN ---
    
    return model

# ---------------- Entrenamiento ----------------
def train_tmo_from_hosting_kaggle_residual():
    print("\n" + "="*70)
    print("ENTRENAMIENTO TMO v7-residual-AFINADO | objetivo = 'TMO (s)' / 'TMO (segundos)'")
    print("="*70)

    # 1) Cargar Hosting
    if not os.path.exists(HOSTING_FILE):
        raise FileNotFoundError(f"No se encuentra el archivo: {HOSTING_FILE}")
    raw = read_any(HOSTING_FILE, sheet=0)

    # 2) ts + columnas clave
    d = ensure_ts_from_hosting(raw)

    # feriados
    if FERIADOS_COL not in d.columns:
        cand = [c for c in raw.columns if _norm(c).replace("_"," ") == FERIADOS_COL]
        if cand:
            d[FERIADOS_COL] = pd.to_numeric(raw[cand[0]], errors='coerce').fillna(0).astype(int)
        else:
            raise ValueError(f"No se encontró columna '{FERIADOS_COL}'.")
    else:
        d[FERIADOS_COL] = pd.to_numeric(d[FERIADOS_COL], errors='coerce').fillna(0).astype(int)

    # TMO objetivo
    tmo_col = find_tmo_col(raw)
    d = d.join(pd.to_numeric(raw[tmo_col], errors="coerce").rename("tmo_s"))
    d["tmo_s"] = pd.to_numeric(d["tmo_s"], errors="coerce")

    # (opcional) columna contestadas (para features y/o sample_weight)
    contest_col = find_col_by_candidates(raw, CONTESTADAS_CANDIDATES)
    if contest_col is not None:
        print(f"INFO: Usando '{contest_col}' como columna de volumen (contestadas).")
        d = d.join(pd.to_numeric(raw[contest_col], errors="coerce").rename("contestadas"))
    else:
        print("WARN: No se encontró columna de 'contestadas'. Los features de volumen serán 0.")
        d["contestadas"] = 0

    # Cargar Contexto
    print("INFO: Buscando columnas de contexto (proporciones, tmo_segmento)...")
    loaded_context_cols = []
    raw_cols_norm = {_norm(c): c for c in raw.columns}
    for cand_norm in CONTEXT_COL_CANDIDATES:
        if cand_norm in raw_cols_norm:
            orig_col = raw_cols_norm[cand_norm]
            print(f"INFO: Cargando columna de contexto: '{orig_col}'")
            d[cand_norm] = pd.to_numeric(raw[orig_col], errors='coerce')
            d[cand_norm] = d[cand_norm].ffill().bfill().fillna(d[cand_norm].mean()) # Relleno robusto
            loaded_context_cols.append(cand_norm)

    # 3) partes de tiempo + baseline por (dow,hour)
    d = add_time_parts(d)
    baseline = (d.groupby(["dow","hour"])["tmo_s"]
                 .median().rename("tmo_baseline")
                 .reset_index())
    d = d.merge(baseline, on=["dow","hour"], how="left")

    # residuo
    d["tmo_resid"] = d["tmo_s"] - d["tmo_baseline"]

    # 4) autorregresivas (Bloque de 3 tipos de features)
    
    # 4a) PISTA 1: Lags/MAs del RESIDUO (La "sorpresa" reciente)
    print("INFO: Creando features Pista 1 (Residual)...")
    for lag in [1,2,3,6,12,24,48,72,168]:
        d[f"lag_resid_{lag}"] = d["tmo_resid"].shift(lag)
    resid_shift1 = d["tmo_resid"].shift(1)
    for w in [6,12,24,72,168]:
        d[f"ma_resid_{w}"] = resid_shift1.rolling(w, min_periods=1).mean()
    for span in [6,12,24]:
        d[f"ema_resid_{span}"] = resid_shift1.ewm(span=span, adjust=False, min_periods=1).mean()
    for w in [24,72]:
        d[f"std_resid_{w}"] = resid_shift1.rolling(w, min_periods=2).std()
        d[f"max_resid_{w}"] = resid_shift1.rolling(w, min_periods=1).max()

    
    # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 1) ---
    # 4b) PISTA 2: Lags/MAs del TMO TOTAL (La "tendencia" reciente)
    print("INFO: Creando features Pista 2 (TMO Total) con lags RÁPIDOS...")
    s_tmo_total = d["tmo_s"]
    # Agregamos lags "rápidos" (1, 2, 3, 6, 12) para reaccionar a picos/valles
    for lag in [1, 2, 3, 6, 12, 24, 168]:
        d[f"lag_tmo_total_{lag}"] = s_tmo_total.shift(lag)
    s_tmo_total_s1 = s_tmo_total.shift(1)
    # Agregamos MAs "rápidas" (6, 12)
    for w in [6, 12, 24, 72]:
        d[f"ma_tmo_total_{w}"] = s_tmo_total_s1.rolling(w, min_periods=1).mean()
    # --- FIN MODIFICACIÓN ---

    # 4c) PISTA 3: Lags/MAs de VOLUMEN (El "contexto" de llamadas)
    print("INFO: Creando features Pista 3 (Volumen/Contestadas)...")
    s_contest = pd.to_numeric(d["contestadas"], errors="coerce").fillna(0)
    for lag in [1, 24, 48, 168]:
         d[f"lag_contest_{lag}"] = s_contest.shift(lag)
    s_contest_s1 = s_contest.shift(1)
    for w in [6, 24, 72]:
        d[f"ma_contest_{w}"] = s_contest_s1.rolling(w, min_periods=1).mean()


    # 5) features (Lista de features combinada)
    resid_feats = [c for c in d.columns if c.startswith("lag_resid_") or c.startswith("ma_resid_")]
    ema_feats   = [f"ema_resid_{s}" for s in [6,12,24]]
    vol_feats   = [f"std_resid_{w}" for w in [24,72]] + [f"max_resid_{w}" for w in [24,72]]
    cyc_feats   = ["sin_hour","cos_hour","sin_dow","cos_dow", FERIADOS_COL, "es_dia_de_pago", "dow","month","hour"]
    
    # Listas de features nuevas
    tmo_total_feats = [c for c in d.columns if c.startswith("lag_tmo_total_") or c.startswith("ma_tmo_total_")]
    contest_feats   = [c for c in d.columns if c.startswith("lag_contest_") or c.startswith("ma_contest_")]
    context_feats   = loaded_context_cols # Las columnas de contexto que cargamos

    features = (
        resid_feats + ema_feats + vol_feats + cyc_feats + 
        tmo_total_feats + contest_feats + context_feats
    )
    
    print(f"\nINFO: Total features a usar: {len(features)}")

    # 6) dataset modelable (objetivo = z-score del residuo)
    dm = d.dropna(subset=features + ["tmo_resid", "tmo_baseline"]).copy()
    # estandarización de objetivo (residuo)
    resid_mean = dm["tmo_resid"].mean()
    resid_std  = dm["tmo_resid"].std(ddof=0) or 1.0
    dm["tmo_resid_z"] = (dm["tmo_resid"] - resid_mean) / resid_std

    X = pd.get_dummies(dm[features], columns=["dow","month","hour"], drop_first=False)
    y = dm["tmo_resid_z"].loc[X.index].astype(float)

    # sample_weight (opcional, no es feature)
    if USE_SAMPLE_WEIGHT and "contestadas" in dm.columns:
        print("INFO: Usando SAMPLE WEIGHT (ponderado por 'contestadas')")
        sw = dm["contestadas"].loc[X.index].fillna(0).astype(float)
        sw = np.sqrt(np.clip(sw, 0, np.percentile(sw[sw>0], 99))) + 1.0  # suavizado + cap p99
    else:
        print("INFO: NO se usará sample weight (entrenamiento uniforme).")
        sw = None

    # saneo
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    if X.isna().any().any():
        print("INFO: Rellenando NaNs en features (ffill)...")
        X = X.ffill().dropna()
    y = y.loc[X.index]
    if sw is not None:
        sw = sw.loc[X.index]
    if X.empty:
        raise ValueError("No quedaron datos para entrenar TMO tras el preprocesamiento.")

    # 7) split temporal + escalado
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, shuffle=False)
    scaler = StandardScaler()
    X_tr_s = scaler.fit_transform(X_tr)
    X_te_s = scaler.transform(X_te)
    sw_tr = sw.loc[X_tr.index].values if sw is not None else None
    sw_te = sw.loc[X_te.index].values if sw is not None else None

    # 8) modelo
    model = create_model(X_tr_s.shape[1])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, mode='min')
    model.fit(
        X_tr_s, y_tr,
        validation_data=(X_te_s, y_te, sw_te) if sw_te is not None else (X_te_s, y_te),
        sample_weight=sw_tr,
        epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, callbacks=[es]
    )

    # 9) métricas (reconstruyendo a segundos)
    yhat_z = model.predict(X_te_s, verbose=0).flatten()
    yhat_resid = yhat_z * resid_std + resid_mean
    # baseline para test
    base_te = dm.loc[X_te.index, "tmo_baseline"].values
    yhat_sec = base_te + yhat_resid
    y_true_sec = dm.loc[X_te.index, "tmo_s"].values

    mae = mean_absolute_error(y_true_sec, yhat_sec)
    r2  = r2_score(y_true_sec, yhat_sec)
    print(f"\nResultado TMO v7-residual-AFINADO: MAE={mae:.2f} | R2={r2:.3f}")
    print(f"Distrib y_te : med={np.median(y_true_sec):.1f} | p90={np.percentile(y_true_sec,90):.1f} | p99={np.percentile(y_true_sec,99):.1f}")
    print(f"Distrib pred: med={np.median(yhat_sec):.1f} | p90={np.percentile(yhat_sec,90):.1f} | p99={np.percentile(yhat_sec,99):.1f}")

    # 10) artefactos
    out = Path(OUTPUT_DIR); out.mkdir(parents=True, exist_ok=True)
    # columnas + scaler de features
    (out / "training_columns_tmo.json").write_text(json.dumps(list(X.columns), ensure_ascii=False, indent=2), encoding="utf-8")
    joblib.dump(scaler, out / "scaler_tmo.pkl")
    # baseline por (dow,hour)
    base_df = baseline.sort_values(["dow","hour"])
    base_df.to_csv(out / "tmo_baseline_dow_hour.csv", index=False)
    # meta de residuo
    meta = {
        "resid_mean": float(resid_mean),
        "resid_std":  float(resid_std),
        "use_sample_weight": bool(USE_SAMPLE_WEIGHT),
        "contestadas_col_used": contest_col,
        # --- INICIO MODIFICACIÓN (AFINADO: Ajuste 2) ---
        "loss": "mean_squared_error", # <-- Actualizado
        "delta": None # <-- Huber ya no se usa
        # --- FIN MODIFICACIÓN ---
    }
    (out / "tmo_residual_meta.json").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
    # modelo
    model.save(str(out / "modelo_tmo.keras"))  # en inferencia: load_model(..., compile=False)

    print("\nArtefactos guardados en:", str(out.resolve()))
    print(" - modelo_tmo.keras")
    print(" - scaler_tmo.pkl")
    print(" - training_columns_tmo.json")
    print(" - tmo_baseline_dow_hour.csv")
    print(" - tmo_residual_meta.json")
    print("\n¡Listo! Descárgalos desde la pestaña 'Output'.")

# ---------------- Run ----------------
if __name__ == "__main__":
    train_tmo_from_hosting_kaggle_residual()
