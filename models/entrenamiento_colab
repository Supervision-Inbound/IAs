# Celda 1: Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

--------------------------------------------------------------------------------------------
# Celda 2: Forzar la actualización de todo
!pip install --upgrade pandas numpy tensorflow scikit-learn joblib requests requests-cache dill

----------------------------------------------------------------------------------------------
# 2. Ahora el script puede importarla y chequear la versión
import pandas
import numpy
import tensorflow
import sklearn
import joblib
import requests
import keras
import requests_cache
import dill

print("--- Pega esto en tu requirements.txt de GitHub ---")
print(f"pandas=={pandas.__version__}") # Usar __version__ es más estándar
print(f"numpy=={numpy.__version__}")   # Usar __version__ es más estándar
print(f"tensorflow=={tensorflow.__version__}")
print(f"keras=={keras.__version__}")
print(f"scikit-learn=={sklearn.__version__}")
print(f"joblib=={joblib.__version__}")
print(f"requests=={requests.__version__}")
print(f"dill=={dill.__version__}")
print(f"requests-cache=={requests_cache.__version__}") # ¡Aquí está el cambio!
print("--------------------------------------------------")

------------------------------------------------------------------------------------------------------------

# =================================================================================
# PIPELINE MAESTRO DE ENTRENAMIENTO v4 (COLAB - SIN VOLATILIDAD NI DIA DE PAGO)
# 1. Combina los 3 entrenamientos (Planner, TMO v8, Riesgos v7).
# 2. GUARDA TODOS LOS ARTEFACTOS EN /content/ARTEFACTOS IA/
# 3. ELIMINADA la lógica 'es_dia_de_pago' de todos los modelos.
# 4. Comprime la carpeta y la descarga automáticamente.
# =================================================================================

import os
import re
import json
import joblib
import shutil
import unicodedata
import warnings
from pathlib import Path
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import regularizers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    mean_absolute_error, r2_score, roc_auc_score,
    accuracy_score, precision_score, recall_score,
    confusion_matrix
)
from google.colab import files

# --- CONFIGURACIÓN GLOBAL DE ENTRADA (DRIVE) ---
DRIVE_INPUT_DIR = "/content/drive/MyDrive/data-ia/"
HOSTING_FILE = os.path.join(DRIVE_INPUT_DIR, "Hosting ia.xlsx")
CLIMA_FILE = os.path.join(DRIVE_INPUT_DIR, "historico_clima.csv")
TMO_FILE = os.path.join(DRIVE_INPUT_DIR, "TMO_HISTORICO.csv") 

# --- CONFIGURACIÓN GLOBAL DE SALIDA (COLAB) ---
MAIN_OUTPUT_DIR = "/content/ARTEFACTOS IA/"
PLANNER_OUTPUT_DIR = os.path.join(MAIN_OUTPUT_DIR, "planner")
RIESGOS_OUTPUT_DIR = os.path.join(MAIN_OUTPUT_DIR, "riesgos")
TMO_OUTPUT_DIR = os.path.join(MAIN_OUTPUT_DIR, "tmo")

# --- CONFIGURACIÓN DE MODELOS ---
EPOCHS = 100 
BATCH_SIZE = 256
SEED = 42
TARGET_CALLS = "recibidos_nacional"
TARGET_TMO_v7 = "tmo_general" 
FERIADOS_COL = "feriados"

np.random.seed(SEED)
tf.random.set_seed(SEED)
warnings.filterwarnings("once", category=UserWarning)

# =================================================================================
# SECCIÓN 1: FUNCIONES DE UTILIDAD (Combinadas)
# =================================================================================

def read_any(path: str, sheet: int | str | None = None) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Archivo no encontrado: {path}")
    p = str(path).lower()
    if p.endswith(".csv"):
        try:
            df = pd.read_csv(path, low_memory=False)
            if df.shape[1] == 1 and ';' in str(df.iloc[0,0]):
                df = pd.read_csv(path, sep=';', low_memory=False)
            return df
        except Exception:
            return pd.read_csv(path, sep=';', low_memory=False)
    if p.endswith(".xlsx") or p.endswith(".xls"):
        return pd.read_excel(path, sheet_name=sheet if sheet is not None else 0)
    raise ValueError(f"Formato no soportado: {path}")

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def ensure_ts_v8(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d.columns = [c.strip().lower().replace(" ", "_") for c in d.columns]
    fecha_col = next((c for c in d.columns if "fecha" in c), None)
    hora_col  = next((c for c in d.columns if "hora"  in c), None)
    if not fecha_col or not hora_col:
        raise ValueError("Se requieren columnas con 'fecha' y 'hora' para construir 'ts'.")
    d["ts"] = pd.to_datetime(d[fecha_col].astype(str) + " " + d[hora_col].astype(str),
                             errors="coerce", dayfirst=True)
    d = d.dropna(subset=["ts"]).sort_values("ts").reset_index(drop=True)
    return d
    
def ensure_ts_v7(df):
    df.columns = [c.lower().strip().replace(' ', '_') for c in df.columns]
    date_col = next((c for c in df.columns if 'fecha' in c), None)
    hour_col = next((c for c in df.columns if 'hora' in c), None)
    if not date_col or not hour_col:
        raise ValueError("No se encontraron 'fecha' y 'hora'.")
    df["ts"] = pd.to_datetime(df[date_col].astype(str) + ' ' + df[hour_col].astype(str), errors='coerce')
    return df.dropna(subset=["ts"]).sort_values("ts")

def find_tmo_col(df: pd.DataFrame) -> str:
    norm_map = {_norm(c): c for c in df.columns}
    for n, orig in norm_map.items():
        if re.match(r"^tmo\s*\(\s*(s|segundos)\s*\)$", n):
            return orig
    for n, orig in norm_map.items():
        if "tmo" in n and ("(s)" in n or "segundo" in n):
            return orig
    raise ValueError("No se encontró columna TMO v8: usa 'TMO (s)' o 'TMO (segundos)'.")

def add_time_parts(df: pd.DataFrame) -> pd.DataFrame:
    d = df.copy()
    d["dow"]   = d["ts"].dt.dayofweek
    d["month"] = d["ts"].dt.month
    d["hour"]  = d["ts"].dt.hour
    d["day"]   = d["ts"].dt.day
    d["sin_hour"] = np.sin(2*np.pi*d["hour"]/24.0)
    d["cos_hour"] = np.cos(2*np.pi*d["hour"]/24.0)
    d["sin_dow"]  = np.sin(2*np.pi*d["dow"]/7.0)
    d["cos_dow"]  = np.cos(2*np.pi*d["dow"]/7.0)
    # <-- MODIFICADO: 'es_dia_de_pago' eliminado
    return d

def find_col_by_candidates(df: pd.DataFrame, candidates: list) -> str | None:
    cols_norm = {_norm(c): c for c in df.columns}
    for cand in candidates:
        n = _norm(cand)
        for k, orig in cols_norm.items():
            if k == n or k.replace("_", " ") == n:
                return orig
    if "contestadas" in candidates:
         for k, orig in cols_norm.items():
            if "contest" in k or ("recibidos" in k and "nacional" in k):
                return orig
    return None

def normalize_climate_columns(df):
    column_map = {'temperatura': ['temperature_2m', 'temperatura'], 'precipitacion': ['precipitation', 'precipitacion', 'precipitación'], 'lluvia': ['rain', 'lluvia']}
    df_renamed = df.copy()
    for standard_name, possible_names in column_map.items():
        for name in possible_names:
            if name in df_renamed.columns:
                df_renamed.rename(columns={name: standard_name}, inplace=True)
                break
    return df_renamed

def create_planner_risk_model(n_features, loss='mean_squared_error', output_bias=None, is_classifier=False):
    if output_bias is not None:
        output_bias = tf.keras.initializers.Constant(output_bias)
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(256, activation='relu', input_shape=(n_features,)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid' if is_classifier else 'linear', bias_initializer=output_bias)
    ])
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.001, decay_steps=10000, decay_rate=0.9)
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)
    metrics = [tf.keras.metrics.AUC(name='auc')] if is_classifier else ['mae']
    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)
    return model

def create_tmo_model(n_features: int) -> tf.keras.Model:
    l2_reg = 1e-6 
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(n_features,)),
        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(0.20),
        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dropout(0.15),
        tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),
        tf.keras.layers.Dense(1, activation='linear'),
    ])
    lr = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9
    )
    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)
    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mae'])
    return model

# =================================================================================
# SECCIÓN 2: FUNCIONES DE ENTRENAMIENTO
# =================================================================================

# --- FASE 1: PLANIFICADOR (LLAMADAS) ---
def train_planner_model(df_hosting, output_dir):
    print("\n" + "="*50); print("--- FASE 1: ENTRENANDO MODELO 1 (EL PLANIFICADOR) v3 ---"); print("="*50)
    df_base = add_time_parts(df_hosting.copy())
    # <-- MODIFICADO: 'es_dia_de_pago' eliminado
    for lag in [24, 48, 72, 168]:
        df_base[f'lag_{lag}'] = df_base[TARGET_CALLS].shift(lag)
    for window in [24, 72, 168]:
        df_base[f'ma_{window}'] = df_base[TARGET_CALLS].rolling(window, min_periods=1).mean()
    df_base.dropna(inplace=True)
    
    # <-- MODIFICADO: 'es_dia_de_pago' eliminado de la lista de features
    features_base = [col for col in df_base.columns if col.startswith(('lag_', 'ma_'))] + ['sin_hour', 'cos_hour', 'sin_dow', 'cos_dow', 'feriados', 'dow', 'month', 'hour']
    
    X = pd.get_dummies(df_base[features_base], columns=['dow', 'month', 'hour'])
    y = df_base[TARGET_CALLS]
    
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, shuffle=False)
    
    scaler = StandardScaler()
    X_tr_s = scaler.fit_transform(X_tr)
    X_te_s = scaler.transform(X_te)
    
    model = create_planner_risk_model(X_tr_s.shape[1], is_classifier=False)
    model.fit(X_tr_s, y_tr, validation_data=(X_te_s, y_te), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, 
              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)])
    
    pred = model.predict(X_te_s).flatten()
    print(f"\nResultado Planificador v3: MAE={mean_absolute_error(y_te, pred):.2f} | R2={r2_score(y_te, pred):.3f}")
    
    os.makedirs(output_dir, exist_ok=True)
    model.save(os.path.join(output_dir, "modelo_planner.keras"))
    joblib.dump(scaler, os.path.join(output_dir, "scaler_planner.pkl"))
    with open(os.path.join(output_dir, "training_columns_planner.json"), "w") as f:
        json.dump(list(X.columns), f)
        
    print(f"Artefactos del Planificador guardados en: {output_dir}")
    return model, scaler, list(X.columns)

# --- FASE 2: ANALISTA DE RIESGOS (CLIMA) ---
def train_risk_analyst_model(df_hosting, df_clima, planner_model, planner_scaler, planner_cols, output_dir):
    print("\n" + "="*50); print("--- FASE 2: ENTRENANDO MODELO 2 (EL ANALISTA DE RIESGOS) v7 ---"); print("="*50)
    
    df_planner_full = add_time_parts(df_hosting.copy())
    # <-- MODIFICADO: 'es_dia_de_pago' eliminado
    for lag in [24, 48, 72, 168]: df_planner_full[f'lag_{lag}'] = df_planner_full[TARGET_CALLS].shift(lag)
    for window in [24, 72, 168]: df_planner_full[f'ma_{window}'] = df_planner_full[TARGET_CALLS].rolling(window, min_periods=1).mean()
    df_planner_full.dropna(inplace=True)
    
    X_full_dummies = pd.get_dummies(df_planner_full, columns=['dow', 'month', 'hour'])
    X_full = X_full_dummies.reindex(columns=planner_cols, fill_value=0)
    X_full_s = planner_scaler.transform(X_full)
    df_planner_full['llamadas_predichas_normales'] = planner_model.predict(X_full_s)
    df_planner_full['pico_de_llamadas'] = (df_planner_full[TARGET_CALLS] - df_planner_full['llamadas_predichas_normales']).clip(0)
    
    pico_threshold = df_planner_full['pico_de_llamadas'].quantile(0.95)
    print(f"Umbral para definir un 'evento de alto volumen': {pico_threshold:.2f} llamadas adicionales.")
    df_planner_full['evento_de_alto_volumen'] = (df_planner_full['pico_de_llamadas'] > pico_threshold).astype(int)
    df_picos = df_planner_full[['ts', 'evento_de_alto_volumen']].copy()
    
    df_clima_proc = add_time_parts(df_clima.copy())
    df_clima_proc = df_clima_proc.sort_values(['comuna', 'ts'])
    climate_cols_to_fill = ['temperatura', 'precipitacion', 'lluvia']
    for col in climate_cols_to_fill:
        if col in df_clima_proc.columns: df_clima_proc[col] = df_clima_proc.groupby('comuna')[col].ffill()
    df_clima_proc.dropna(subset=climate_cols_to_fill, inplace=True)
    weather_metrics = [m for m in climate_cols_to_fill if m in df_clima_proc.columns]
    if not weather_metrics: raise ValueError("Ninguna columna de clima esperada fue encontrada.")
    
    print("Calculando baselines de clima...")
    baselines = df_clima_proc.groupby(['comuna', 'dow', 'hour'])[weather_metrics].agg(['median', 'std']).reset_index()
    baselines.columns = ['_'.join(col).strip() for col in baselines.columns.values]; std_cols = [col for col in baselines.columns if col.endswith('_std')]; baselines[std_cols] = baselines[std_cols].fillna(0)
    df_clima_proc = pd.merge(df_clima_proc, baselines, left_on=['comuna', 'dow', 'hour'], right_on=['comuna_', 'dow_', 'hour_'])
    for metric in weather_metrics:
        median_col = f'{metric}_median'; std_col = f'{metric}_std'
        df_clima_proc[f'anomalia_{metric}'] = (df_clima_proc[metric] - df_clima_proc[median_col]) / (df_clima_proc[std_col] + 1e-6)
    
    anomaly_cols = [f'anomalia_{m}' for m in weather_metrics]; n_comunas = df_clima_proc['comuna'].nunique()
    agg_functions = {col: ['max', 'sum', lambda x: (x > 2.5).sum() / n_comunas] for col in anomaly_cols}
    df_agregado = df_clima_proc.groupby('ts').agg(agg_functions).reset_index()
    df_agregado.columns = [f"{col[0]}_{col[1] if col[1] != '<lambda_0>' else 'pct_comunas_afectadas'}" for col in df_agregado.columns]
    
    df_final_anomaly = pd.merge(df_agregado, df_picos, left_on='ts_', right_on='ts')
    X = df_final_anomaly.drop(columns=['ts_', 'ts', 'evento_de_alto_volumen'])
    y = df_final_anomaly['evento_de_alto_volumen']
    
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, shuffle=False)
    scaler = StandardScaler(); X_tr_s = scaler.fit_transform(X_tr); X_te_s = scaler.transform(X_te)
    
    neg, pos = np.bincount(y_tr)
    total = neg + pos
    weight_for_0 = (1 / neg) * (total / 2.0)
    weight_for_1 = (1 / pos) * (total / 2.0)
    class_weight = {0: weight_for_0, 1: weight_for_1}
    print(f"Pesos de clase calculados: 0 -> {class_weight[0]:.2f}, 1 -> {class_weight[1]:.2f}")
    
    initial_bias = np.log([pos / neg])
    model = create_planner_risk_model(X_tr_s.shape[1], loss='binary_crossentropy', output_bias=initial_bias, is_classifier=True)
    
    model.fit(X_tr_s, y_tr, validation_data=(X_te_s, y_te), epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1,
              class_weight=class_weight,
              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)])
    
    pred_proba = model.predict(X_te_s).flatten()
    pred_class = (pred_proba > 0.5).astype(int)
    
    print("\n--- Resultados del Analista de Riesgos v7 ---")
    print(f"AUC: {roc_auc_score(y_te, pred_proba):.3f}")
    print(f"Recall: {recall_score(y_te, pred_class, zero_division=0):.3f}")
    print(confusion_matrix(y_te, pred_class))
    
    os.makedirs(output_dir, exist_ok=True)
    model.save(os.path.join(output_dir, "modelo_riesgos.keras"))
    joblib.dump(scaler, os.path.join(output_dir, "scaler_riesgos.pkl"))
    joblib.dump(baselines, os.path.join(output_dir, "baselines_clima.pkl"))
    with open(os.path.join(output_dir, "training_columns_riesgos.json"), "w") as f:
        json.dump(list(X.columns), f)
        
    print(f"Artefactos de Riesgos guardados en: {output_dir}")
    return df_agregado

# --- FASE 3: ANALISTA DE OPERACIONES (TMO v8 - SIN VOLATILIDAD) ---
def train_tmo_model(hosting_file_path, output_dir):
    print("\n" + "="*70)
    print("ENTRENAMIENTO TMO v8-DIRECTO (SIN VOLATILIDAD NI CONTEXTO)")
    print("="*70)

    # 1) Cargar Hosting
    raw = read_any(hosting_file_path, sheet=0)

    # 2) ts + columnas clave
    d = ensure_ts_v8(raw)
    
    # feriados
    feriados_col_orig = find_col_by_candidates(raw, [FERIADOS_COL])
    if feriados_col_orig:
        d[FERIADOS_COL] = pd.to_numeric(raw[feriados_col_orig], errors='coerce').fillna(0).astype(int)
    else:
        raise ValueError(f"No se encontró columna '{FERIADOS_COL}'.")

    # TMO objetivo
    tmo_col = find_tmo_col(raw)
    d = d.join(pd.to_numeric(raw[tmo_col], errors="coerce").rename("tmo_s"))
    d["tmo_s"] = pd.to_numeric(d["tmo_s"], errors="coerce")

    # columna contestadas
    contest_col = find_col_by_candidates(raw, ["contestadas", "q_contestadas", "recibidos_nacional"])
    if contest_col:
        print(f"INFO: Usando '{contest_col}' como columna de volumen (contestadas).")
        d = d.join(pd.to_numeric(raw[contest_col], errors="coerce").rename("contestadas"))
    else:
        print("WARN: No se encontró columna de 'contestadas'. Los features de volumen serán 0.")
        d["contestadas"] = 0
    
    # 3) partes de tiempo
    d = add_time_parts(d)

    # 4) features autorregresivas TMO
    print("INFO: Creando features Pista 1 (TMO Total) con lags RÁPIDOS...")
    s_tmo_total = d["tmo_s"]
    for lag in [1, 2, 3, 6, 12, 24, 168]:
        d[f"lag_tmo_total_{lag}"] = s_tmo_total.shift(lag)
    s_tmo_total_s1 = s_tmo_total.shift(1)
    for w in [6, 12, 24, 72]:
        d[f"ma_tmo_total_{w}"] = s_tmo_total_s1.rolling(w, min_periods=1).mean()

    # 5) features autorregresivas Volumen
    print("INFO: Creando features Pista 2 (Volumen/Contestadas)...")
    s_contest = pd.to_numeric(d["contestadas"], errors="coerce").fillna(0)
    for lag in [1, 24, 48, 168]:
         d[f"lag_contest_{lag}"] = s_contest.shift(lag)
    s_contest_s1 = s_contest.shift(1)
    for w in [6, 24, 72]:
        d[f"ma_contest_{w}"] = s_contest_s1.rolling(w, min_periods=1).mean()

    # 6) features
    # <-- MODIFICADO: 'es_dia_de_pago' eliminado de la lista
    cyc_feats    = ["sin_hour","cos_hour","sin_dow","cos_dow", FERIADOS_COL, "dow","month","hour"]
    tmo_total_feats = [c for c in d.columns if c.startswith(("lag_tmo_total_", "ma_tmo_total_"))]
    contest_feats   = [c for c in d.columns if c.startswith(("lag_contest_", "ma_contest_"))]
    
    features = (cyc_feats + tmo_total_feats + contest_feats)
    
    print(f"\nINFO: Total features TMO a usar: {len(features)}")

    # 7) dataset modelable
    dm = d.dropna(subset=features + ["tmo_s"]).copy()
    X = pd.get_dummies(dm[features], columns=["dow","month","hour"], drop_first=False)
    y = dm["tmo_s"].loc[X.index].astype(float)
    
    # Sample weight
    if "contestadas" in dm.columns:
        print("INFO: Usando SAMPLE WEIGHT (ponderado por 'contestadas')")
        sw = dm["contestadas"].loc[X.index].fillna(0).astype(float)
        sw = np.sqrt(np.clip(sw, 0, np.percentile(sw[sw>0], 99))) + 1.0
    else:
        sw = None

    # Saneo
    X.replace([np.inf, -np.inf], np.nan, inplace=True)
    X = X.ffill().dropna()
    y = y.loc[X.index]
    if sw is not None: sw = sw.loc[X.index]

    # 8) split + escalado
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.20, shuffle=False)
    scaler = StandardScaler()
    X_tr_s = scaler.fit_transform(X_tr)
    X_te_s = scaler.transform(X_te)
    sw_tr = sw.loc[X_tr.index].values if sw is not None else None
    sw_te = sw.loc[X_te.index].values if sw is not None else None

    # 9) modelo
    model = create_tmo_model(X_tr_s.shape[1])
    es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=15, restore_best_weights=True, mode='min')
    model.fit(
        X_tr_s, y_tr,
        validation_data=(X_te_s, y_te, sw_te) if sw_te is not None else (X_te_s, y_te),
        sample_weight=sw_tr,
        epochs=120, batch_size=BATCH_SIZE, verbose=1, callbacks=[es]
    )

    # 10) métricas
    yhat_sec = model.predict(X_te_s, verbose=0).flatten()
    y_true_sec = y_te.values
    print(f"\nResultado TMO v8-DIRECTO (SIN VOLATILIDAD): MAE={mean_absolute_error(y_true_sec, yhat_sec):.2f} | R2={r2_score(y_true_sec, yhat_sec):.3f}")

    # 11) artefactos
    out = Path(output_dir); out.mkdir(parents=True, exist_ok=True)
    (out / "training_columns_tmo.json").write_text(json.dumps(list(X.columns), ensure_ascii=False, indent=2), encoding="utf-8")
    joblib.dump(scaler, out / "scaler_tmo.pkl")
    model.save(str(out / "modelo_tmo.keras"))

    print(f"Artefactos de TMO guardados en: {output_dir}")
    
# =================================================================================
# SECCIÓN 3: ORQUESTADOR PRINCIPAL
# =================================================================================

def main():
    print("Iniciando pipeline maestro de entrenamiento...")
    
    # --- 1. Limpiar y crear directorios de salida ---
    if os.path.exists(MAIN_OUTPUT_DIR):
        shutil.rmtree(MAIN_OUTPUT_DIR)
    os.makedirs(PLANNER_OUTPUT_DIR, exist_ok=True)
    os.makedirs(RIESGOS_OUTPUT_DIR, exist_ok=True)
    os.makedirs(TMO_OUTPUT_DIR, exist_ok=True)
    print(f"Directorio principal de artefactos creado en: {MAIN_OUTPUT_DIR}")

    # --- 2. Cargar datos comunes ---
    print("Cargando datos iniciales (Hosting y Clima)...")
    
    df_hosting_raw = read_any(HOSTING_FILE, sheet=0)
    df_hosting = ensure_ts_v7(df_hosting_raw)
    df_hosting = df_hosting.rename(columns={'recibidos': TARGET_CALLS})
    
    feriados_col_orig = find_col_by_candidates(df_hosting_raw, [FERIADOS_COL])
    if feriados_col_orig:
        df_hosting[FERIADOS_COL] = pd.to_numeric(df_hosting_raw[feriados_col_orig], errors='coerce').fillna(0).astype(int)
    else:
        raise ValueError(f"No se encontró '{FERIADOS_COL}' en Hosting.xlsx.")
        
    df_hosting = df_hosting.groupby("ts").agg({TARGET_CALLS: 'sum', FERIADOS_COL: 'max'}).reset_index()

    df_clima_raw = read_any(CLIMA_FILE)
    df_clima = ensure_ts_v7(df_clima_raw)
    df_clima = normalize_climate_columns(df_clima)

    max_date_calls = df_hosting['ts'].max()
    print(f"Alineando datos de clima a la última fecha de llamadas: {max_date_calls}")
    df_clima = df_clima[df_clima['ts'] <= max_date_calls]

    # --- 3. Ejecutar entrenamientos en secuencia ---
    
    # Fase 1: Planificador
    planner_model, planner_scaler, planner_cols = train_planner_model(
        df_hosting, PLANNER_OUTPUT_DIR
    )
    
    # Fase 2: Riesgos
    train_risk_analyst_model(
        df_hosting, df_clima, 
        planner_model, planner_scaler, planner_cols, 
        RIESGOS_OUTPUT_DIR
    )

    # Fase 3: TMO v8
    train_tmo_model(
        HOSTING_FILE, TMO_OUTPUT_DIR
    )

    # --- 4. Comprimir y descargar todo ---
    print("\n" + "="*50); print("--- FASE 4: EMPAQUETANDO Y DESCARGANDO TODO ---"); print("="*50)
    
    zip_file_name = "/content/ARTEFACTOS_IA_COMPLETOS"
    shutil.make_archive(zip_file_name, 'zip', MAIN_OUTPUT_DIR)
    
    print(f"¡Todos los artefactos comprimidos en {zip_file_name}.zip!")
    print("Iniciando descarga... Por favor, acepta la descarga en tu navegador.")
    
    files.download(f"{zip_file_name}.zip")

# --- Ejecutar el pipeline ---
if __name__ == "__main__":
    main()
